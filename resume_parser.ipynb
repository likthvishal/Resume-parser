{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tika\n",
        "from tika import parser\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import multiprocessing\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Initialize Tika for OCR\n",
        "tika.initVM()\n",
        "\n",
        "# Load SpaCy's pre-trained NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# please add your custom dataset files\n",
        "GROUND_TRUTH = {\n",
        "    \"resume1.pdf\": {\"name\": \"likith vishal\", \"email\": \"likithvishal@gmail.com\"},\n",
        "    \"resume2.pdf\": {\"name\": \"Liki vish\", \"email\": \"likithvishal20@gmail.com\"},\n",
        "\n",
        "}\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    raw = parser.from_file(file_path)\n",
        "    return raw['content']\n",
        "\n",
        "def process_resume(text)\n",
        "    # Parse the text using SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenization, lemmatization, POS tagging, and stopword removal\n",
        "    tokens = [(token.text, token.lemma_, token.pos_, token.is_stop) for token in doc]\n",
        "\n",
        "    # Named Entity Recognition (NER)\n",
        "    entities = {ent.label_: ent.text for ent in doc.ents}\n",
        "\n",
        "    return tokens, entities\n",
        "\n",
        "def visualize_entities(doc):\n",
        "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
        "\n",
        "def calculate_entity_accuracy(entities, ground_truth):\n",
        "    correct_entities = sum(entities.get(label) == ground_truth.get(label) for label in ground_truth)\n",
        "    total_entities = len(ground_truth)\n",
        "    accuracy = correct_entities / total_entities if total_entities > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "def process_resume_file(file_path, ground_truth):\n",
        "    # Extract text from the resume\n",
        "    text = extract_text_from_pdf(file_path)\n",
        "\n",
        "    # Process the text (tokenization, lemmatization, POS tagging, NER)\n",
        "    tokens, entities = process_resume(text)\n",
        "\n",
        "    # Calculate extraction accuracy based on ground truth\n",
        "    accuracy = calculate_entity_accuracy(entities, ground_truth)\n",
        "\n",
        "    # Print tokens, entities, and accuracy\n",
        "    print(f\"Resume: {file_path}\")\n",
        "    print(\"Tokens (word, lemma, POS, stopword):\")\n",
        "    for token in tokens:\n",
        "        print(token)\n",
        "\n",
        "    print(\"\\nNamed Entities:\")\n",
        "    for label, entity in entities.items():\n",
        "        print(f\"{label}: {entity}\")\n",
        "\n",
        "    print(f\"\\nExtraction Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "    # Visualize entities\n",
        "    doc = nlp(text)\n",
        "    visualize_entities(doc)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def parallel_resume_processing(file_paths):\n",
        "    ground_truths = [GROUND_TRUTH.get(os.path.basename(file), {}) for file in file_paths]\n",
        "\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        accuracies = pool.starmap(process_resume_file, zip(file_paths, ground_truths))\n",
        "\n",
        "    return accuracies\n",
        "\n",
        "def measure_processing_speed(file_paths):\n",
        "    start_time = time.time()\n",
        "    parallel_resume_processing(file_paths)\n",
        "    end_time = time.time()\n",
        "\n",
        "    total_time = end_time - start_time\n",
        "    resumes_per_hour = (len(file_paths) / total_time) * 3600\n",
        "\n",
        "    print(f\"Processed {len(file_paths)} resumes in {total_time:.2f} seconds.\")\n",
        "    print(f\"Speed: {resumes_per_hour:.2f} resumes per hour.\")\n",
        "\n",
        "# Test the parser with a batch of resumes\n",
        "file_paths = [\"resume1.pdf\", \"resume2.pdf\"]  # Replace with actual resume paths\n",
        "measure_processing_speed(file_paths)\n",
        "\n",
        "# Calculate average accuracy for the batch\n",
        "accuracies = parallel_resume_processing(file_paths)\n",
        "average_accuracy = sum(accuracies) / len(accuracies)\n",
        "print(f\"\\nAverage Extraction Accuracy: {average_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "wNkxFGkF2GZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}